next up
  stuff from notes
    add merge log table and insert them in merge()
    move merge stuff into fs/merge.py?

  big ***s in latus.py
    handle merge errors, especially "created!" and "changed!"

  get a network up and running
    login
    send messages
    ask for latest metadata
    diff a diff of the metadata
    fetch chunks
    merge fetched chunks

  fetch using chunk store
    implement fetch_chunks(entries)
      there should be a queue of the diffs
	* it should be keyed by the path so we aren't doing an "old action"
	but, it should have a separate queue item for each chunk?
      uses a "coordinator" and FileChunkSource

    implement merge_entries(entries):
      touches file or moves file or copies file
      gets temp file ready, moves current to trash, moves temp in place.
      * inserts history entry with author_peerid and author_utime

      failures:
	if fs has changed, notify scanner
	if no sources working, ???
	if fs error moving file in place, LOG and leave chunks around

    implement FileChunkSource:
      read_chunk(chunkid) -> chunk_data
      * needs a (hash, size) => current [(path, mtime, size)] lookup

    implement FileChunkStore
      ChunkId(Record("hash", "loc", "size"))
      stores in db for restarts
      caches in memory
      get_status(chunkid)
      begin(chunk) -> token (probably time)
      cancel(chunkid, token)
      write(chunkid, data)
      read(chunkid) -> data  # ?    
      remove(chunkid)

    implement downloading algo
      could use a priority queue

      def begin_fetch_write(chunkid):
	token = chunk_store.begin(chunkid)
	try:
	  data = ** fetch **
	except:
	  chunk_store.cancel(chunkid, token)
	  ** handle error **
	else:
	  chunk_store.write(chunkid, data)

      while chunk_queue:
	chunkid = chunk_queue.get()
	state = chunk_store.get_status(chunkid)
	if state == complete:
	  pass  # great!
	elif state == inprogress
	  inprogress_chunk_queue.put(chunkid)
	else:
	  begin_fetch_write(chunkid)

      while inprogress_chunk_queue:
	chunkid = chunk_queue.get()
	if state == complete:
	  pass  # great!
	else:  # even if in progress
	  begin_fetch_write(chunkid)

  add gropuid and security
  **** get photos to Cess

  cleanup?
      fs: fs -> history
        FileSystem
      history: storing, comparing, reading history
        split up scanning into getting + updating
	  updating is in history
	  getting is in fs
      sync: getting remote history, fetching, merging
      net: xmpp, nat punch, firewall, sockets
  more good notes
    status log (slog)
      put in db
      put in logging module (file)
      track status of hashing
    status of hashing/fetching:
      finished_count, total_count,
      finished_size, total_size,
      current_path, current_finished_size, current_size
    write status to .latus/status.txt constantly
    need from peer:
      read_entries(since)
      read_chunk(hash, loc, size)
      read_merge_log(since)
      read_status()

  got some syncing going on!
    Implement meta->meta one way sync
      use peerid
    Implement more xmpp stuff
    watch out for performance
    implement trash, especially for updates
    figure out what to do with conflicts

  check TODOs in files

  from notes
    put trash and conflicting versions in the same place
      that means just put conflicting files in trash?
      also put prepared version in trash?
      make the trash a ChunkSource?
      is it a content-addressable-store where things can disappear?
        but should we bother data de-dupping
      for "revisions", you could just copy even before it's deleted locally
      pros:
        easy for user to see/delete/cleanup
	pretty simple
	covers trash, revisions, and conflicts
	keeps us safe from remote deletes
	keeps us safe from remote changes
    log tables
      runtimes: (action, start, duration, result)
      fetchqueue: (peerid, groupid, path, size, mtime, hash, utime)
        (just in memory?)
      fetchedchunks: hash, loc, size, utime
      mergelog: (action=copy|moveto|movefrom|delete|create|replace,
                 path, old_mtime, old_size, old_hash,
		       new_mtime, new_size, new_hash,
		       author_peerid, author_utime, other_path)
    main tables
      files
      filechunks?
    log files
      untsable files
      ignored files (but not every time!)
      non-files (but not every time!)
    watchers
      need coalesing, at least a little?
        to capture deleted+created=move
	to avoid performance issues when someone is writing to a file that's unstable?
    for status
      sources
	files metadata from local and remote
	mergelog from local and remote
	fetchqueue from local and remote
	fetchedchunks from local and remote (maybe)
	some way of knowing speed, possibly as return result of fetchqueue
	  calculated by how fast we transfer files, recorded by fetchers
      show:
        1. number of files + total size
	2. last N merge operations
	3. "synced" or "downloading N files M size ETA XYZ"
	  with list of files?
	  ETA = size / speed
      for now, update by constantly writing to a stauts file
        
    inotify:
      evey scan, get scanned dirs, and add a watch for each one
	keep track of previously added ones
	dont' auto-add or rec
      when a file changes, trigger a scan, and be sure to ignore correctly
      for OSX, just do directories, but maybe calcualte "important" files to watch
	(priority by modtime?)

    big stuff left to figure out
      1. groups: how to separate them
      2. permissions: control how sees what, and who we merge from
      3. p2p networking and discovery
      4. status and logging
      5. UI
      6. file watching

  next steps
    *** sync with iriver using it
      no hashing?
      must have efficient moves and copies

    make FileWatcher
      pyinotify
        make it faster
          no auto_add
          no rec
          add after scanning, every time :)
            scan only directories, not files
      osx
        make it cover directories
        and maybe later, smart files
      win32
        make it work again

    get an xmpp client that serves requests for metadata
    get an xmpp client that requests metadata and copies it

  unit tests
    tests/FileSystemTests.py
    tests/FileHistoryTests.py
    tests/FileScannerTests.py: uses InMemoryFileSystem, InMemoryHistoryStore
    tests/UtilTests.py
    
  latus.py improvements
    logging instead of printing
    logging passed to FileSystem
    slow down disk IO to not clobber system when hashing a lot
  
  only 4 FS threads:
    1. history: reads + writes db
    2. scanner: stats + hashes
    3. merger: writes/copies/moves
    4. chunk source: reads

overall:
  xmpp<->xmpp messages
  xmpp<->xmpp metadata syncing for one user
  p2p fs->fs 1-way sync for one user
  p2p fs<-fs 2-way sync

xmpp:
  send a custom message
  receive custom messages
  keep track of who is available
  spawn a gevent "thread" to talk to each one until unavailable
  p2p data (w/ SSL!)

concurency:
  try gevent first
  then try concurrence

improvements later
  after fetching and merging
  commit in chunks of 1,000 so files start syncing even while
    scanning the first time (which will take a long time)
  add file watchers (win32 and inotify for now)
  filter by Win32 attributes?

python
  io module
  binary literals
  class decorators
  run zips directly
  next(itr, default)
  new set methods
  collections.namedtuple
  collections.OrderedDict
  itertools product, combinations, etc
  logging.RotatingFileHandler
  os.walk w/ followLinks
  os.path.relpath
  os.path.expandvars: ~peter and %VAR%
  PriorityQueue
  json module
  plistlib module
  ssl
  dict comprehensions
  memoryview
  argparse module
  dictionary views
  collections.Counter
  Fraction
  unittest improvements assertAlmostEquals, assertItemsEqual, etc
  
