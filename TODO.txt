Big TODOs
  memoize all ignore_pattern returns, not just the false ones!
    and test the speed again
  slow down disk IO to not clobber system when hashing a lot
  simplify RunTimer/RunTime (just roll into StatusLog?)

  verify bytes after copying?
  logging instead of printing
  if revisions_root = None, just delete?
  revisions with groupids?
  use peerid in HistoryStore select!
  use peerid in mergelog select
  handle merge errors, especially "created!" and "changed!"
    if fs has changed, notify scanner
    if no sources working, ???
    if fs error moving file in place, LOG and leave chunks around

group scanning
  implement reading of .psync files
  implement parent/child groups
  implement groupid security
       
get a network up and running
  diff a diff of the metadata
  fetch chunks
  merge fetched chunks

unit tests
  tests/FileSystemTests.py
  tests/FileHistoryTests.py
  tests/FileScannerTests.py: uses InMemoryFileSystem, InMemoryHistoryStore
  tests/UtilTests.py
    
ideas
  different threads
    one thread reading/writing sockets, using asyncore
    one thread listening for incoming connections
    one thread connecting to peers (try every 5 min or so)
    one thread reading/write xmpp socket (it's special)
    one thread fs scans and history updates
    one thread doing remote updates/scans/diffs/merging
      a separate thread for fetching?


  fetch using chunk store
    fetch into "revisions" (once done)

    implement fetch_chunks(entries)
      there should be a queue of the diffs
	* it should be keyed by the path so we aren't doing an "old action"
	but, it should have a separate queue item for each chunk?
      uses a "coordinator" and FileChunkSource

    implement FileChunkSource:
      read_chunk(chunkid) -> chunk_data
      * needs a (hash, size) => current [(path, mtime, size)] lookup

    implement FileChunkStore
      ChunkId(Record("hash", "loc", "size"))
      stores in db for restarts
      caches in memory
      get_status(chunkid)
      begin(chunk) -> token (probably time)
      cancel(chunkid, token)
      write(chunkid, data)
      read(chunkid) -> data  # ?    
      remove(chunkid)

    implement downloading algo
      could use a priority queue

      def begin_fetch_write(chunkid):
	token = chunk_store.begin(chunkid)
	try:
	  data = ** fetch **
	except:
	  chunk_store.cancel(chunkid, token)
	  ** handle error **
	else:
	  chunk_store.write(chunkid, data)

      while chunk_queue:
	chunkid = chunk_queue.get()
	state = chunk_store.get_status(chunkid)
	if state == complete:
	  pass  # great!
	elif state == inprogress
	  inprogress_chunk_queue.put(chunkid)
	else:
	  begin_fetch_write(chunkid)

      while inprogress_chunk_queue:
	chunkid = chunk_queue.get()
	if state == complete:
	  pass  # great!
	else:  # even if in progress
	  begin_fetch_write(chunkid)

    status log (slog)
      put in db
      put in logging module (file)
      track status of hashing
    status of hashing/fetching:
      finished_count, total_count,
      finished_size, total_size,
      current_path, current_finished_size, current_size
    write status to .psync/status.txt constantly
    need from peer:
      read_entries(since)
      read_chunk(hash, loc, size)
      read_merge_log(since)
      read_status()

    log tables
      runtimes: (action, start, duration, result)
      fetchqueue: (peerid, groupid, path, size, mtime, hash, utime)
        (just in memory?)
      fetchedchunks: hash, loc, size, utime
      more mergelog fields? old_mtime, old_size, old_hash,
                            new_mtime, new_size, new_hash,
			    author_utime
    watchers
      need coalesing, at least a little?
        to capture deleted+created=move
	to avoid performance issues when someone is writing to a file that's unstable?
    for status
      sources
	files metadata from local and remote
	mergelog from local and remote
	fetchqueue from local and remote
	fetchedchunks from local and remote (maybe)
	some way of knowing speed, possibly as return result of fetchqueue
	  calculated by how fast we transfer files, recorded by fetchers
      show:
        1. number of files + total size
	2. last N merge operations
	3. "synced" or "downloading N files M size ETA XYZ"
	  with list of files?
	  ETA = size / speed
      for now, update by constantly writing to a stauts file
        
    inotify:
      evey scan, get scanned dirs, and add a watch for each one
	keep track of previously added ones
	dont' auto-add or rec
      when a file changes, trigger a scan, and be sure to ignore correctly
      for OSX, just do directories, but maybe calcualte "important" files to watch
	(priority by modtime?)

    big stuff left to figure out
      - permissions: control how sees what, and who we merge from
      - p2p networking and discovery
      - status and logging
      - UI
      - file watching

   sync with iriver using it
      no hashing?

    make FileWatcher
      pyinotify
        make it faster
          no auto_add
          no rec
          add after scanning, every time :)
            scan only directories, not files
      osx
        make it cover directories
        and maybe later, smart files
      win32
        make it work again

improvements later
  commit in chunks of 1,000 so files start syncing even while
    scanning the first time (which will take a long time)
  add file watchers (win32 and inotify for now)
  filter by Win32 attributes?

python 2.6 things to try
  io module
  binary literals
  class decorators
  run zips directly
  next(itr, default)
  new set methods
  collections.namedtuple
  collections.OrderedDict
  itertools product, combinations, etc
  logging.RotatingFileHandler
  os.walk w/ followLinks
  os.path.relpath
  os.path.expandvars: ~peter and %VAR%
  PriorityQueue
  json module
  plistlib module
  ssl
  dict comprehensions
  memoryview
  argparse module
  dictionary views
  collections.Counter
  Fraction
  unittest improvements assertAlmostEquals, assertItemsEqual, etc
  
